{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> 5447744\n"
     ]
    }
   ],
   "source": [
    "with open('data/shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(type(text), len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7469\n",
      "3599\n"
     ]
    }
   ],
   "source": [
    "docs = re.split(r'\\n\\n', text)\n",
    "print(len(docs))\n",
    "docs = [d for d in docs if d.count('\\n') > 1]\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eod_token = 'eod'\n",
    "\n",
    "def clean_docs(text, eod_token='eod'):\n",
    "    text = re.sub(r'[\\d`{}|&_<>%àûïêîëèä$*/æé#@]', '', text)\n",
    "    text = '\\n'.join([l.strip() for l in text.split('\\n') if l.strip() != '']) + ' ' + eod_token\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [clean_docs(d) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_boundary_pattern = r'([\\s,!?:;.-])'\n",
    "\n",
    "def tokenize_string(s):\n",
    "    return [t for t in re.split(token_boundary_pattern, s) if t not in ['', ' ']]\n",
    "\n",
    "def tokenize(docs):\n",
    "    sequences = [tokenize_string(d) for d in docs]\n",
    "    word_count = Counter([t for s in sequences for t in s])\n",
    "    return sequences, word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27971\n",
      "[('\\n', 107574), (',', 80860), ('.', 76515), ('the', 26808), ('and', 25621), ('i', 20144), ('to', 19287), ('of', 17809), (';', 17076), ('a', 14117), ('you', 13589), ('my', 12453), ('that', 11112), ('in', 10859), ('?', 10459), ('is', 9548), ('!', 8800), ('not', 8707), ('for', 8190), ('-', 7797), ('me', 7761), ('it', 7661), ('with', 7612), ('be', 7068), ('your', 6863), ('this', 6794), ('his', 6726), ('but', 6262), ('he', 6200), ('as', 5882), ('have', 5873), ('thou', 5474), ('so', 5256), ('him', 5132), ('will', 4963), ('what', 4452), ('by', 4354), ('thy', 4028), ('all', 3880), ('are', 3832), ('her', 3796), ('no', 3768), ('do', 3747), ('eod', 3599), ('shall', 3579), ('if', 3481), ('we', 3284), ('thee', 3180), ('or', 3054), ('our', 3054), ('on', 3028), ('lord', 3000), ('good', 2808), ('now', 2778), ('king', 2739), ('sir', 2685), ('from', 2617), ('come', 2498), ('at', 2451), ('they', 2386), ('which', 2315), ('would', 2288), ('more', 2286), ('well', 2228), ('was', 2227), ('o', 2227), ('then', 2207), ('she', 2197), ('am', 2167), ('how', 2158), ('love', 2097), ('here', 2095), ('let', 2090), ('their', 2048), ('when', 2038), ('them', 1960), ('hath', 1939), ('than', 1880), ('man', 1855), ('may', 1853), ('there', 1810), ('an', 1795), (':', 1774), ('like', 1767), ('one', 1756), (\"i'll\", 1740), ('go', 1729), ('upon', 1723), ('say', 1674), ('know', 1648), ('make', 1630), ('did', 1625), ('us', 1615), ('such', 1611), ('were', 1574), ('yet', 1572), ('should', 1571), ('must', 1491), ('why', 1469), ('see', 1437)]\n"
     ]
    }
   ],
   "source": [
    "sequences, word_count = tokenize(docs)\n",
    "print(len(word_count))\n",
    "print(word_count.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(word_count, vocab_size):\n",
    "    padding_token = '#'\n",
    "    word_inverted_index = [padding_token] + sorted([w[0] for w in word_count.most_common(vocab_size)])\n",
    "    word_index = {w: i for w, i in zip(word_inverted_index, range(len(word_inverted_index)))}    \n",
    "    return word_index, word_inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27972\n"
     ]
    }
   ],
   "source": [
    "word_index, word_inverted_index = create_vocab(word_count, len(word_count))\n",
    "vocab_size = len(word_index)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9149\n"
     ]
    }
   ],
   "source": [
    "eod_index = word_index[eod_token]\n",
    "print(eod_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_encode_sequence(sequence, word_index):\n",
    "    return [word_index[w] for w in sequence if w in word_index]\n",
    "\n",
    "def index_encode(sequences, word_index):\n",
    "    return [index_encode_sequence(s, word_index) for s in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_sequence(sequence, word_index, window_size, padding_index=0):\n",
    "    # Encode\n",
    "    sequence = index_encode_sequence(sequence, word_index)\n",
    "    # Pad\n",
    "    padding = (window_size - len(sequence))\n",
    "    if padding >= 0:\n",
    "        return [padding_index] * padding + sequence\n",
    "    else:\n",
    "        return sequence[-window_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'fairest', 'creatures', 'we', 'desire', 'increase', ',', '\\n', 'that', 'thereby', \"beauty's\", 'rose', 'might', 'never', 'die', ',', '\\n', 'but', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ',', '\\n', 'his', 'tender', 'heir', 'might', 'bear', 'his', 'memory', ':', '\\n', 'but', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', ',', '\\n', \"feed'st\", 'thy', \"light's\", 'flame', 'with', 'self', '-', 'substantial', 'fuel', ',', '\\n', 'making', 'a', 'famine', 'where', 'abundance', 'lies', ',', '\\n', 'thy', 'self', 'thy', 'foe', ',', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', ':', '\\n', 'thou', 'that', 'art', 'now', 'the', \"world's\", 'fresh', 'ornament', ',', '\\n', 'and', 'only', 'herald', 'to', 'the', 'gaudy', 'spring', ',', '\\n', 'within', 'thine', 'own', 'bud', 'buriest', 'thy', 'content', ',', '\\n', 'and', 'tender', 'churl', \"mak'st\", 'waste', 'in', 'niggarding', ':', '\\n', 'pity', 'the', 'world', ',', 'or', 'else', 'this', 'glutton', 'be', ',', '\\n', 'to', 'eat', 'the', \"world's\", 'due', ',', 'by', 'the', 'grave', 'and', 'thee', '.', 'eod']\n",
      "3599\n",
      "[10808, 9652, 6590, 27084, 7461, 13146, 784, 1, 24469, 24511, 3028, 20966, 15788, 16643, 7614, 784, 1, 4304, 2325, 24477, 20829, 22077, 4345, 24787, 7125, 784, 1, 12431, 24373, 12227, 15788, 2985, 12431, 15640, 787, 1, 4304, 24604, 6150, 24848, 24556, 17567, 4058, 9584, 784, 1, 9906, 24738, 14638, 10195, 27534, 21689, 785, 23664, 10847, 784, 1, 15209, 1131, 9718, 27257, 1227, 14612, 784, 1, 24738, 21689, 24738, 10361, 784, 24848, 24738, 24015, 21689, 24910, 6706, 787, 1, 24604, 24469, 2302, 16854, 24477, 27683, 10759, 17337, 784, 1, 1925, 17226, 12284, 24848, 24477, 11069, 23085, 784, 1, 27552, 24556, 17567, 4188, 4261, 24738, 6121, 784, 1, 1925, 24373, 5210, 15198, 27025, 13074, 16679, 787, 1, 18521, 24477, 27682, 784, 17288, 8757, 24585, 11333, 2961, 784, 1, 24848, 8620, 24477, 27683, 8472, 784, 4345, 24477, 11561, 1925, 24482, 786, 9149]\n",
      "3599\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])\n",
    "print(len(sequences))\n",
    "sequences = index_encode(sequences, word_index)\n",
    "print(sequences[0])\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "stride = 1\n",
    "padding_size = window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window(sequence, window_size, stride, padding_size, padding_index=0):\n",
    "    padded = [padding_index] * padding_size + sequence\n",
    "    windows = []\n",
    "    start_pos = 0\n",
    "    end_pos = window_size + 1\n",
    "    while end_pos < len(padded):\n",
    "        windows.append(padded[start_pos:end_pos])\n",
    "        start_pos += stride\n",
    "        end_pos += stride\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200133, 21)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([w for s in sequences for w in window(s, window_size, stride, padding_size)])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200133,)\n",
      "(1200133, 20)\n"
     ]
    }
   ],
   "source": [
    "y = X[:, -1]\n",
    "print(y.shape)\n",
    "X = X[:, :-1]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200133, 27972)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_cat = to_categorical(y)\n",
    "print(y_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27972"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = max([max(seq) for seq in sequences if len(seq) > 0]) + 1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_path = 'data/embeddings/glove.6B.50d.txt.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with gzip.open(glove_path, 'r') as fin:\n",
    "#    line = fin.readline().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def parse_line(line):\n",
    "#    values = line.decode('utf-8').strip().split()\n",
    "#    word = values[0]\n",
    "#    vector = np.asarray(values[1:], dtype='float32')\n",
    "#    return word, vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = {}\n",
    "#word_index = {}\n",
    "#word_inverted_index = []\n",
    "\n",
    "#with gzip.open(glove_path, 'r') as fin:\n",
    "#    for idx, line in enumerate(fin):\n",
    "#        word, vector = parse_line(line) # parse a line\n",
    "        \n",
    "#        embeddings[word] = vector  # add word vector\n",
    "#        word_index[word] = idx  # add idx\n",
    "#        word_inverted_index.append(word)  # append word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = len(embeddings)\n",
    "#emb_size = len(embeddings['good'])\n",
    "#print(vocab_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import CuDNNLSTM, Embedding\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_weights = np.zeros((vocab_size, emb_size))\n",
    "#for word, index in word_index.items():\n",
    "#    embedding_weights[index, :] = embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_layer = Embedding(\n",
    "#    input_dim=vocab_size,\n",
    "#    output_dim=emb_size,\n",
    "#    weights=[embedding_weights],\n",
    "#    mask_zero=False,\n",
    "#    trainable=False,\n",
    "#    input_length=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "save_best = ModelCheckpoint('../models/sp_word_weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 32)            895104    \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 128)               82944     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 27972)             3608388   \n",
      "=================================================================\n",
      "Total params: 4,586,436\n",
      "Trainable params: 4,586,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "#model.add(emb_layer)\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=window_size))\n",
    "model.add(CuDNNLSTM(128, return_sequences=False))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=Adam(), \n",
    "    #optimizer=RMSprop(lr=0.01), \n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1080119 samples, validate on 120014 samples\n",
      "Epoch 1/10\n",
      "1080119/1080119 [==============================] - 352s 326us/step - loss: 6.0815 - acc: 0.1126 - val_loss: 5.8944 - val_acc: 0.1384\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.89440, saving model to ../models/sp_word_weights.01-5.89.hdf5\n",
      "Epoch 2/10\n",
      "1080119/1080119 [==============================] - 321s 297us/step - loss: 5.4555 - acc: 0.1716 - val_loss: 5.5298 - val_acc: 0.1689\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.89440 to 5.52980, saving model to ../models/sp_word_weights.02-5.53.hdf5\n",
      "Epoch 3/10\n",
      "1080119/1080119 [==============================] - 322s 298us/step - loss: 5.1473 - acc: 0.1977 - val_loss: 5.3616 - val_acc: 0.1863\n",
      "\n",
      "Epoch 00003: val_loss improved from 5.52980 to 5.36157, saving model to ../models/sp_word_weights.03-5.36.hdf5\n",
      "Epoch 4/10\n",
      "1080119/1080119 [==============================] - 321s 297us/step - loss: 4.9704 - acc: 0.2145 - val_loss: 5.2644 - val_acc: 0.2011\n",
      "\n",
      "Epoch 00004: val_loss improved from 5.36157 to 5.26443, saving model to ../models/sp_word_weights.04-5.26.hdf5\n",
      "Epoch 5/10\n",
      "1080119/1080119 [==============================] - 322s 298us/step - loss: 4.8372 - acc: 0.2274 - val_loss: 5.2047 - val_acc: 0.2075\n",
      "\n",
      "Epoch 00005: val_loss improved from 5.26443 to 5.20474, saving model to ../models/sp_word_weights.05-5.20.hdf5\n",
      "Epoch 6/10\n",
      "1080119/1080119 [==============================] - 325s 301us/step - loss: 4.7329 - acc: 0.2363 - val_loss: 5.1517 - val_acc: 0.2174\n",
      "\n",
      "Epoch 00006: val_loss improved from 5.20474 to 5.15170, saving model to ../models/sp_word_weights.06-5.15.hdf5\n",
      "Epoch 7/10\n",
      "1080119/1080119 [==============================] - 330s 305us/step - loss: 4.6472 - acc: 0.2430 - val_loss: 5.1307 - val_acc: 0.2184\n",
      "\n",
      "Epoch 00007: val_loss improved from 5.15170 to 5.13066, saving model to ../models/sp_word_weights.07-5.13.hdf5\n",
      "Epoch 8/10\n",
      "1080119/1080119 [==============================] - 327s 303us/step - loss: 4.5734 - acc: 0.2482 - val_loss: 5.1015 - val_acc: 0.2265\n",
      "\n",
      "Epoch 00008: val_loss improved from 5.13066 to 5.10151, saving model to ../models/sp_word_weights.08-5.10.hdf5\n",
      "Epoch 9/10\n",
      "1080119/1080119 [==============================] - 323s 299us/step - loss: 4.5070 - acc: 0.2527 - val_loss: 5.0939 - val_acc: 0.2266\n",
      "\n",
      "Epoch 00009: val_loss improved from 5.10151 to 5.09390, saving model to ../models/sp_word_weights.09-5.09.hdf5\n",
      "Epoch 10/10\n",
      "1080119/1080119 [==============================] - 326s 302us/step - loss: 4.4467 - acc: 0.2565 - val_loss: 5.0939 - val_acc: 0.2259\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 5.09390\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f658804e240>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y_cat, epochs=10, batch_size=1024, verbose=1, validation_split=0.1, callbacks=[early_stop, save_best])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('../models/sp_word_weights.09-5.09.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(p, diversity=1.0):\n",
    "    p1 = np.asarray(p).astype('float64')\n",
    "    p1 = np.log(p1) / diversity\n",
    "    e_p1 = np.exp(p1)\n",
    "    s = np.sum(e_p1)\n",
    "    p1 = e_p1 / s\n",
    "    return np.argmax(np.random.multinomial(1, p1, 1))\n",
    "\n",
    "def sequence_to_string(sequence):\n",
    "    seq = [s if s in ['.', ',', '!', '?', '-', ':', ';', '\\n'] else ' ' + s for s in sequence]\n",
    "    return ''.join(seq)\n",
    "\n",
    "def gen_doc(seed='', max_len=400, diversity=1.0):\n",
    "    # convert seed to padded sequence\n",
    "    doc = tokenize_string(seed)\n",
    "    pred = -1\n",
    "    while pred != eod_index and len(doc) < max_len:\n",
    "        pred = sample(model.predict(np.array([prep_sequence(doc, word_index, window_size)]))[0], diversity)\n",
    "        doc.append(word_inverted_index[pred])\n",
    "    return sequence_to_string(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou contracted to thine own bright eyes,\n",
      "feed'st thy light's flame with self-substantial fuel,\n",
      "making a famine where abundance lies,\n",
      "thy self thy foe, to thy sweet self too cruel:\n",
      "thou that art now the world's fresh ornament,\n",
      "and only herald to the gaudy spring,\n",
      "within thine own bud buriest thy content,\n",
      "and tender churl mak'st waste in niggarding:\n",
      "pity the world, or else this glutton be,\n",
      "to eat the world's due, by the grave and thee. eod\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " from fairest creatures, that i beseech you,\n",
      " which i am sure of your honour,\n",
      " and that you have been so long to die,\n",
      " and so i have not a man; and i am,\n",
      " that is a man of a man, and art you.\n",
      " exit. ]\n",
      " dromio of syracuse. i am full of the world;\n",
      " for i will go with me, and you shall be\n",
      " in a house, and so i do.\n",
      " lear. then, good madam,\n",
      " that i would have been, and that i have seen\n",
      " the body of your own enemies.\n",
      " othello. i am a man;\n",
      " that is as such a man as your highness.\n",
      " antipholus of ephesus. nay, i will tell you.\n",
      " othello. i have been such a man as you to do.\n",
      " buckingham. ay, sir, i will not speak to the best.\n",
      " petruchio. i will not think you to the man of his name.\n",
      " but the gods, my lord, would you have it\n",
      " to make a great behalf of my lord;\n",
      " and i have pass'd to th' law of the\n",
      " to the king of the duke of the king,\n",
      " for i am a man, and that well not.\n",
      " othello. i have not a man to be a man.\n",
      " ham. the king, and the king, my lord, you are a man;\n",
      " and that that he will not, as thou shalt be\n",
      " in all the prince of fortune, but i do not your great\n",
      " and a man of this place; for there is not too?\n",
      " i will have a gentleman of a man.\n",
      " why, i will not be with him to the queen of\n",
      " the world that might have been a little hour of the\n",
      " and that i do not both; for this will i do\n",
      " have been a man and the other.\n",
      " king. i will go.\n",
      " shallow. i am not a man.\n",
      " rosalind. i am\n"
     ]
    }
   ],
   "source": [
    "print(gen_doc('from fairest creatures', diversity=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
